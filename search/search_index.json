{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"EDIA","text":""},{"location":"#a-unity-xr-toolbox-for-research","title":"A Unity XR Toolbox for research","text":"<p>EDIA is designed to reduce implementation overhead in XR experiments by providing reusable architectural patterns and components. It supports rapid iteration and a maintainable project structure, while leaving study-specific interaction logic and Unity scene configuration under the user\u2019s control.</p> <p> The EDIA toolbox is a collection of modules (Unity packages) that help you design and run experimental research in XR.</p> <p></p>"},{"location":"#key-features","title":"Key features","text":"<ul> <li> <p> Structure your experiment</p> <p>Define the information for your experiment at the granularity level which you need: from sessions to blocks to trials to single steps within trials. EDIA builds upon and extends the functionalities of the UXF \u2014 Unity Experiment Framework (Brookes et al., 2020) package.</p> <p> Getting started</p> </li> <li> <p> Manage it with config files</p> <p>Use human readable config files to provide and change central information for your experiment without needing to recompile or touch your Unity code. </p> <p> Overview</p> </li> <li> <p> Unified eye tracking integration</p> <p>EDIA provides an integration of eye tracking for multiple different headsets (HTC Vive, Varjo, Meta Quest Pro, Pico), managing the parsing of the eye tracker output for you in the original sampling rate, and providing it via a standardized interface.</p> <p> <code>EDIA Eye</code></p> </li> <li> <p> Remotely control mobile XR experiments</p> <p>Control experiments which run on a mobile XR headset (e.g., Meta Quest) externally and stream what the participant is seeing.</p> <p> <code>EDIA RCAS</code></p> </li> <li> <p> Automatically log relevant data</p> <p>Use pre-configured tools to log different kinds of data: behavior and movements of the participant, experimental variables, eye tracking data, ...</p> <p> Getting started</p> </li> <li> <p> Synchronize with external data</p> <p>Use the LabStreamingLayer protocol to synchronize your experiment with other data streams (e.g., EEG, fNIRS, ...).</p> <p> <code>EDIA LSL</code></p> </li> </ul>"},{"location":"#getting-started","title":"Getting started","text":"<p>Read our getting started guide. To install the EDIA modules into your Unity project, we recommend using our <code>EDIA Installer</code> ( download,  README,   source). </p>"},{"location":"#modules","title":"Modules","text":"EDIA Core EDIA LSL EDIA RCAS EDIA Eye EDIA Eye Submodules <ul> <li>The \ud83d\udda4 heart of the EDIA toolbox.</li> <li>Structure your experiment: <code>Sessions</code> &lt;&gt; <code>Blocks</code> &lt;&gt; <code>Trials</code> &lt;&gt; <code>Trial Steps</code></li> <li>Use logically nested Config files (JSON) to manage the compiled experiment.</li> <li>Send messages to the XR user.</li> <li>Experimenter interface.</li> <li>Download from  \u2192  <code>EDIA Core</code></li> </ul> <ol> <li>Use the LabStreamingLayer protocol to synchronize your data.</li> <li>The  <code>EDIA LSL</code> module is a convenience wrapper and extension of the LSL4Unity package.</li> <li>It provides prefabs and scripts which allow you to<ol> <li>Send precisely timed triggers with a single command. </li> <li>Easily stream data (eye tracking data, movement data) in world or local coordinates.</li> </ol> </li> <li>Download from  \u2192  <code>EDIA LSL</code></li> </ol> <ol> <li>Remote Control And Streaming.</li> <li>Interface with your experiment which runs on a mobile headset.</li> <li>Load Config files from another device.</li> <li>Send commands to proceed in the experiment.</li> <li>Stream the headset view to the experimenter's device (*limited).</li> <li>Download from  \u2192  <code>EDIA RCAS</code></li> </ol> <ul> <li>Central  eye tracking package of the <code>EDIA toolbox</code>.  </li> <li>Provides a unified interface for accessing eye tracking data.  </li> <li>Example visualizations of eye tracking data: gaze, eye openness, ...  </li> <li>Interface for logging eye tracking data via <code>EDIA LSL</code> or writing it to disc.</li> <li>Download from  \u2192  <code>EDIA Eye</code></li> </ul> <p>Separate submodules (packages) allow to parse the eye tracking data from the respective device into the unified EDIA eye tracking structure. Supported headsets:</p> <p> Meta Quest Pro</p> <p> HTC Vive Pro Eye</p> <p>  Varjo Aero</p> <p>  PICO 4 Enterprise</p> <p></p>"},{"location":"#get-involved","title":"Get involved","text":"<p>If you want to use EDIA or start contributing, please  reach out to us.</p>"},{"location":"apiref/","title":"API reference: EDIA Core","text":"<p>Open external website.</p>"},{"location":"contact/","title":"About EDIA","text":""},{"location":"contact/#contact-us","title":"Contact us","text":"<p>If you have any questions or comments regarding <code>EDIA</code>, please reach out to us. </p> <p> edia.toolbox@gmail.com</p> <p> @ediatoolbox.bsky.social</p> <p> GitHub Discussion Board (will go public soon)</p>"},{"location":"contact/#maintainers","title":"Maintainers","text":"<p>Jeroen de Mooij \u2014 website | LinkedIn Felix Klotzsche \u2014 website | LinkedIn</p>"},{"location":"contact/#attribution","title":"Attribution","text":"<p><code>EDIA</code> was and is mainly developed and supported in the context of the MindBodyEmotion Group (led by Michael Gaebler) at the Max Planck Institute for Human Cognitive and Brain Sciences. </p>"},{"location":"gettingstarted/","title":"Getting started","text":"<p>This <code>getting started guide</code> will take you through the process of setting up a small experiment, explaining all features needed to do that with EDIA along the way.</p> <p>The guide is currently hosted on Notion. To open it full screen click here.</p>"},{"location":"usecases/","title":"Use Cases","text":"<p>Below are example research projects that demonstrate how the EDIA modules can be combined.</p>"},{"location":"usecases/#projects","title":"Projects","text":"<ul> <li> <p> EEG decodability of facial expressions and their stereoscopic depth cues in immersive virtual reality </p> Project details <p>Summary: The study integrates immersive virtual reality with EEG and eye tracking to examine face perception under controlled stereoscopic depth conditions. Using time-resolved multivariate decoding, EEG signals recorded in fully immersive VR reliably differentiated facial expressions. Stereoscopic depth cues elicit distinct, decodable neural signatures, while expression decoding remains robust across depth conditions, demonstrating the feasibility of multimodal decoding in 3D environments.  </p> <p>Time: 2022\u20132025  </p> <p>Equipment </p> <ul> <li>Platform: PC VR</li> <li>Headset: HTC Vive Pro Eye</li> <li>Interaction: HTC Vive Pro Controllers</li> </ul> <p>Data modalities</p> <ul> <li> EEG  </li> <li> eye tracking   </li> <li> subjective reports  </li> </ul> <p>EDIA Modules [version]</p> <ul> <li>  EDIA Core [pre-release]</li> <li> EDIA Eye [pre-release]</li> <li> EDIA Eye Vive [pre-release]</li> </ul> <p>Publications &amp; links </p> <ul> <li> Paper [eLife]</li> <li> GitHub repo</li> </ul> </li> <li> <p> AffectTracker: real-time continuous rating of affective experience in immersive virtual reality</p> Project details <p>Summary: Studying dynamics and physiology of affective states in immersive virtual reality. Emotion elicitation through stereoscopic 360\u00b0 videos and simultaneous recordings of subjective ratings and multimodal physiological and eye tracking data.  </p> <p>Time: 2023\u2013ongoing  </p> <p>Equipment</p> <ul> <li>Platform: PC VR</li> <li>Headset: HTC Vive Pro Eye</li> <li>Interaction: HTC Vive Controllers</li> </ul> <p>Data modalities</p> <ul> <li> EEG</li> <li> ECG</li> <li> respiration</li> <li> skin conductance</li> <li> eye tracking </li> <li> head tracking </li> <li> subjective reports </li> </ul> <p>EDIA Modules [version]</p> <ul> <li>  EDIA Core [pre-release]</li> <li> EDIA LSL [pre-release]</li> <li> EDIA Eye [pre-release]</li> <li> EDIA Eye Vive [pre-release]</li> </ul> <p>Publications &amp; links</p> <ul> <li> Paper [Front. Virtual Real.]</li> <li> GitHub \u2013 AffectTracker</li> <li> Thefirstfloor project page</li> </ul> </li> <li> <p> Project 3DIL: Revolutionising Eyewitness Identification \u2014 The 3D Interactive Lineup </p> Project details <p>Summary: The 3D Interactive Lineups (3DIL) project is transforming eyewitness identification through the use of 3D virtual reality (VR). Traditional 2D photo lineups can often lead to mistaken identifications, contributing to wrongful convictions. Our innovative approach uses advanced 3D modeling and neural rendering to create lifelike, interactive facial models, allowing witnesses to view suspects from multiple angles in VR. This project brings together an international team of experts in psychology, computer science, and law enforcement to set a new standard in criminal justice procedures.  </p> <p>Time: 2024\u2013ongoing </p> <p>Equipment</p> <ul> <li>Platform: PC VR</li> <li>Headset: Varjo Aero</li> <li>Interaction: HTC Vive Controllers</li> </ul> <p>Data modalities</p> <ul> <li> eye tracking </li> <li> subjective reports </li> </ul> <p>EDIA Modules </p> <ul> <li>  EDIA Core [pre-release]</li> <li> EDIA Eye [pre-release]</li> <li> EDIA Eye Varjo [pre-release]</li> </ul> <p>Publications &amp; links</p> <ul> <li> Project website</li> </ul> </li> <li> <p> Scientific Eye Tracking Data Quality in immersive Virtual Reality: A Comparison of Tethered and Mobile VR Headsets </p> Project details <p>Summary: This study evaluates the eye-tracking performance of five state-of-the-art virtual reality (VR) headsets to assess their suitability for scientific research. Using data from 24 participants, the authors compare three mobile devices (Meta Quest Pro, PICO 4 Enterprise, HTC Vive Focus 3) and two tethered headsets (HTC Vive Pro Eye, Varjo Aero) on key performance metrics: spatial accuracy, precision, and temporal latency. Eye tracking was measured during fixation and saccade tasks performed with and without head movements, and latency was estimated relative to concurrently recorded electrooculography (EOG).</p> <p>Time: 2025\u2013ongoing  </p> <p>Equipment</p> <ul> <li>Platform:  PC VR  |  mobile VR </li> <li>Headset:  HTC Vive Pro | Varjo Aero |  Meta Quest Pro |  HTC Vive Focus 3 |   PICO 4 Enterprise</li> <li>Interaction: Controllers</li> </ul> <p>Data modalities</p> <ul> <li> eye tracking </li> <li> EOG </li> <li> head tracking</li> </ul> <p>EDIA Modules</p> <ul> <li>  EDIA Core [v0.4]</li> <li> EDIA Eye [v0.0.1]</li> <li> EDIA Eye Varjo [exp-validet]</li> <li> EDIA Eye Vive [exp-validet]</li> <li> EDIA Eye Quest [exp-validet]</li> <li> EDIA Eye Pico [exp-validet]</li> <li> EDIA Eye Focus [exp-validet]</li> <li> EDIA LSL [pre-release]</li> <li> EDIA RCAS [v0.4.0]</li> </ul> <p>Publications &amp; links</p> <ul> <li> Meeting Abstract [VSS 2025]</li> <li> Poster [Psychologie &amp; Gehirn 2025]</li> </ul> </li> </ul>"}]}